# 数据本地存储方式分析报告

## 当前存储架构

### 存储结构
```
data_cache/
├── metadata.csv              # 元数据（缓存索引）
├── access_log.csv            # LRU 访问日志
├── daily_full_*.parquet      # 全量历史数据（每股票 1 个文件）
├── daily_*.parquet           # 日期范围数据（已清理）
└── trade_cal_*.parquet       # 交易日历
```

### 数据格式
- **数据文件**: Parquet（列式存储）
- **元数据**: CSV（文本格式）
- **索引**: DataFrame 索引为 trade_date

---

## 存在的问题

### 问题 1: 文件数量过多 ⚠️

**现状：**
- 每只股票 = 1 个 parquet 文件
- 5000 只股票 = 5000+ 文件

**影响：**
| 问题 | 影响程度 | 说明 |
|------|----------|------|
| 文件系统压力 | 中 | 大量小文件影响文件系统性能 |
| 备份困难 | 中 | 备份 5000+ 文件较慢 |
| 元数据变大 | 中 | metadata.csv 随文件数增长 |
| 打开速度慢 | 低 | 资源管理器加载慢 |

**建议：**
- 按交易所分目录存储 (SSE/SZSE/BJSE)
- 或使用 SQLite 统一管理

---

### 问题 2: 元数据 CSV 性能瓶颈 ⚠️

**现状：**
```python
# 每次保存都读写整个 CSV
def _save_metadata(self):
    self._metadata.to_csv(self.metadata_file, index=False)

def _load_metadata(self):
    return pd.read_csv(self.metadata_file)
```

**影响：**
| 操作 | 5000 条记录 | 50000 条记录 |
|------|------------|-------------|
| 加载时间 | ~0.1 秒 | ~1 秒 |
| 保存时间 | ~0.05 秒 | ~0.5 秒 |

**建议：**
- 使用 SQLite 替代 CSV
- 或实现增量保存

---

### 问题 3: 无数据压缩 ⚠️

**现状：**
- Parquet 默认使用 SNAPPY 压缩
- 但未优化列类型

**当前存储效率：**
```
5012 只股票 × 243 天 ≈ 1,217,916 条记录
总大小：709 MB
平均每条：约 600 字节
```

**优化空间：**
- 使用 ZSTD 压缩（可再减少 30-50%）
- 优化数据类型（float64→float32）

**建议：**
```python
# 优化前
df.to_parquet(path)

# 优化后
df = df.astype({
    'open': 'float32',
    'high': 'float32',
    'low': 'float32',
    'close': 'float32',
    'vol': 'int64',
})
df.to_parquet(path, compression='zstd')
```

---

### 问题 4: 无数据分区 📊

**现状：**
- 每只股票一个完整文件
- 查询需要加载整个文件

**影响：**
- 只需要最近 10 天数据时，也要加载全年数据
- 内存占用大

**建议：**
```
# 按年分区
data_cache/
├── 000001.SZ/
│   ├── 2023.parquet
│   ├── 2024.parquet
│   └── 2025.parquet
```

---

### 问题 5: 并发访问安全 🔒

**现状：**
- 无文件锁机制
- 多线程读写可能冲突

**风险场景：**
```
线程 A: 读取 metadata.csv
线程 B: 写入 metadata.csv  ← 可能导致 A 读取不完整数据
```

**建议：**
- 添加文件锁
- 或使用支持并发的数据库

---

### 问题 6: 无数据版本管理 📝

**现状：**
- 数据更新后直接覆盖
- 无法追溯历史版本

**影响：**
- 数据错误时无法回滚
- 无法对比数据变化

**建议：**
- 保留最近 N 个版本
- 或使用 Git LFS 管理

---

### 问题 7: 查询效率低 🔍

**现状：**
```python
# 查询多只股票需要多次 IO
for ts_code in ts_codes:
    df = cache.get("daily_full", {"ts_code": ts_code})
```

**影响：**
- 批量查询效率低
- 无法利用向量化查询

**建议：**
- 使用 DuckDB 支持 SQL 查询
- 或使用 Parquet 分区 + Predicate Pushdown

---

## 存储效率对比

### 当前方式
| 指标 | 数值 |
|------|------|
| 总大小 | 709 MB |
| 股票数 | 5012 |
| 平均每只 | 145 KB |
| 每条记录 | 600 字节 |

### 优化后预估
| 优化项 | 预计减少 |
|--------|----------|
| ZSTD 压缩 | -30% |
| 类型优化 | -20% |
| 分区存储 | -10% |
| **总计** | **-50%** |

**优化后大小：约 350 MB**

---

## 推荐改进方案

### 方案 A: SQLite + Parquet（推荐）⭐

**架构：**
```
data_cache/
├── cache.db           # SQLite 数据库（元数据 + 索引）
└── data/              # Parquet 数据文件
    ├── SSE/           # 按交易所分区
    ├── SZSE/
    └── BJSE/
```

**优点：**
- ✅ 元数据查询快（SQL）
- ✅ 支持并发访问
- ✅ 事务安全
- ✅ 易于备份

**缺点：**
- ⚠️ 需要迁移现有数据

---

### 方案 B: DuckDB（简单）⭐

**架构：**
```
data_cache/
└── cache.duckdb       # DuckDB 数据库（直接查询 Parquet）
```

**优点：**
- ✅ 直接查询 Parquet 文件
- ✅ 支持 SQL
- ✅ 无需迁移数据
- ✅ 分析查询快

**缺点：**
- ⚠️ 文件数量未减少

---

### 方案 C: 保持当前 + 优化（保守）

**优化项：**
1. 按交易所分目录
2. 使用 ZSTD 压缩
3. 优化数据类型
4. 添加文件锁

**优点：**
- ✅ 改动最小
- ✅ 兼容现有代码

**缺点：**
- ⚠️ 根本问题未解决

---

## 实施建议

### 短期（1-2 周）
1. ✅ 按交易所分目录存储
2. ✅ 添加 ZSTD 压缩
3. ✅ 优化元数据保存（增量）

### 中期（1-2 月）
1. 迁移到 SQLite 元数据
2. 添加文件锁机制
3. 实现数据版本管理

### 长期（3-6 月）
1. 评估 DuckDB 方案
2. 实现分区存储
3. 添加数据压缩优化

---

## 总结

### 当前问题严重程度

| 问题 | 严重程度 | 是否影响使用 |
|------|----------|-------------|
| 文件数量多 | 🟡 中 | 否 |
| 元数据 CSV 性能 | 🟡 中 | 否（<10000 条时） |
| 无数据压缩 | 🟢 低 | 否 |
| 无分区 | 🟢 低 | 否 |
| 并发安全 | 🟡 中 | 多线程时有风险 |
| 无版本管理 | 🟢 低 | 否 |
| 查询效率 | 🟡 中 | 批量查询时慢 |

### 总体评价

**当前存储方式：可用，但有优化空间**

- ✅ 对于个人使用/小批量回测：**完全够用**
- ⚠️ 对于生产环境/大规模回测：**建议优化**

### 推荐行动

1. **立即实施**：按交易所分目录（简单，效果好）
2. **考虑实施**：SQLite 元数据（中等工作量）
3. **暂不实施**：DuckDB（需要评估）

---

**报告日期**: 2026-02-27
**存储规模**: 5012 只股票，709 MB
