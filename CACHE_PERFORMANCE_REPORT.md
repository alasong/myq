# 数据本地存储性能分析报告

## 测试日期
2026-02-27

## 测试环境
- Windows 11
- Python 3.x
- 缓存文件数：~5000
- 缓存大小：~673 MB

---

## 性能测试结果

### 1. 写入性能

| 测试项 | 数据量 | 耗时 | 速度 |
|--------|--------|------|------|
| 单次写入 | 250 条 | 1.33 秒 | 189 条/秒 |
| 重复写入（含删除） | 1000 条 | 1.42 秒 | 704 条/秒 |

**分析：**
- 写入速度可接受（~200 条/秒）
- 主要耗时：Parquet 写入 + 元数据保存
- 重复写入需要删除旧文件，稍慢

---

### 2. 读取性能

| 测试项 | 数据量 | 平均耗时 | 速度 |
|--------|--------|----------|------|
| 缓存命中（完整数据） | 250 条 | 0.033 秒 | 7,678 条/秒 |
| 带过滤读取 | 250 条 | 0.030 秒 | 8,423 条/秒 |

**分析：**
- 读取性能优秀（>7000 条/秒）
- Parquet 格式读取快速
- 索引过滤高效

---

### 3. 元数据操作性能

| 测试项 | 记录数 | 耗时 |
|--------|--------|------|
| 元数据保存 | 5037 条 | 0.045 秒 |
| 缓存报告生成 | 5037 条 | 1.183 秒 |

**分析：**
- 元数据保存速度可接受（~10 万条/秒）
- 缓存报告生成较慢（需要遍历所有文件）

---

### 4. LRU 淘汰性能

| 测试项 | 淘汰数 | 耗时 |
|--------|--------|------|
| LRU 淘汰（触发保护） | ~20 个 | ~0.5 秒 |

**分析：**
- LRU 淘汰需要检查文件系统
- `daily_full` 类型保护正常工作
- 淘汰速度可接受

---

## 识别的性能瓶颈

### 瓶颈 1: 元数据频繁保存 [中]

**问题：**
```python
def set(self, data_type, params, df, is_complete=False):
    # 每次保存都调用 _save_metadata()
    self._save_metadata()  # ← 写入磁盘
```

**影响：**
- 批量写入时元数据频繁保存
- 大量缓存文件时较慢

**建议修复：**
```python
def set_batch(self, items):
    """批量保存，统一写入元数据"""
    for item in items:
        # 保存数据
        ...
    # 统一保存元数据
    self._save_metadata()
```

---

### 瓶颈 2: 缓存报告生成慢 [低]

**问题：**
```python
def get_cache_report(self):
    # 需要遍历所有文件计算大小
    for _, row in self._metadata.iterrows():
        total_size += Path(row["path"]).stat().st_size
```

**影响：**
- 5000+ 文件时报告生成 >1 秒
- 不影响核心功能

**建议修复：**
- 缓存总大小，增量更新
- 异步生成报告

---

### 瓶颈 3: LRU 访问日志保存 [低]

**问题：**
```python
def _update_access_log(self, key):
    self._save_access_log()  # ← 每次访问都保存
```

**影响：**
- 高频访问时频繁写入

**建议修复：**
- 批量保存访问日志
- 定期持久化

---

## 已修复的问题

### Bug 1: daily_full 过滤逻辑错误 ✅

**问题：** 索引类型不匹配导致过滤失败

**修复：**
```python
if not pd.api.types.is_datetime64_any_dtype(df.index):
    try:
        df.index = pd.to_datetime(df.index, format='%Y%m%d')
    except (ValueError, TypeError):
        df.index = pd.to_datetime(df.index)
```

---

### Bug 2: 元数据冗余记录 ✅

**问题：** 同一 key 可能有多条记录

**修复：** 保存前先删除旧记录

---

## 性能优化建议

### 短期优化（1-2 天）

1. **批量保存接口**
   ```python
   def set_batch(self, items, batch_size=10):
       """批量保存，减少元数据保存次数"""
       for i, item in enumerate(items):
           self.set(**item)
           if (i + 1) % batch_size == 0:
               self._save_metadata()
       self._save_metadata()
   ```

2. **延迟保存元数据**
   ```python
   def set(self, ...):
       # 标记需要保存
       self._metadata_dirty = True
       # 不立即保存
       
   def flush_metadata(self):
       """显式刷新元数据"""
       if self._metadata_dirty:
           self._save_metadata()
   ```

### 中期优化（1-2 周）

3. **元数据缓存**
   - 内存缓存总大小
   - 增量更新统计信息

4. **异步日志保存**
   - 访问日志先写入内存
   - 定期异步持久化

### 长期优化（1-2 月）

5. **数据库存储元数据**
   - 使用 SQLite 替代 CSV
   - 提高查询效率

6. **分层缓存**
   - 热数据：内存
   - 温数据：SSD
   - 冷数据：HDD

---

## 总体评价

| 指标 | 评分 | 说明 |
|------|------|------|
| 读取性能 | ⭐⭐⭐⭐⭐ | 优秀（>7000 条/秒） |
| 写入性能 | ⭐⭐⭐⭐ | 良好（~200 条/秒） |
| 元数据操作 | ⭐⭐⭐⭐ | 良好（~10 万条/秒） |
| 缓存保护 | ⭐⭐⭐⭐⭐ | daily_full 保护正常 |
| 扩展性 | ⭐⭐⭐ | 中等（5000+ 文件时稍慢） |

**总体评分：⭐⭐⭐⭐ (4/5)**

---

## 结论

### 当前性能状况

1. **读取性能优秀** - Parquet 格式 + 索引过滤
2. **写入性能可接受** - 主要耗时在 Parquet 写入
3. **元数据管理良好** - 5000+ 记录仍可接受
4. **缓存保护有效** - daily_full 类型不被淘汰

### 主要瓶颈

1. **元数据频繁保存** - 批量操作时影响性能
2. **报告生成慢** - 需要遍历文件系统

### 建议

1. **无需紧急优化** - 当前性能满足使用需求
2. **按需优化** - 如果缓存文件数 >1 万，考虑批量保存
3. **监控性能** - 定期查看缓存报告

---

**报告生成时间**: 2026-02-27
**测试工具**: `quant_strategy/tools/test_perf_simple.py`
